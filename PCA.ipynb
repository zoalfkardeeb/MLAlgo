{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Import Libraries\n",
    "# import tensorflow as tf\n"
   ],
   "id": "594ed922b0a92d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:37:04.421143Z",
     "start_time": "2024-05-03T14:37:03.092552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import TSNE"
   ],
   "id": "9ba1090e16e9860f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# import umap\n",
    "#...........................................................\n",
    "#........Introduction.......................................\n",
    "# In this work PCA for dimensionally reduction is applied\n",
    "#  MNIST dataset is used.  MNIST contains\n",
    "# 28*28 images of handwritten digits. The goal is to show that not all\n",
    "#  28*28=784 features are needed to classify the digits.\n",
    "#..........................................................\n",
    "# Loading mnist train dataset and dividing it into x_train and y train"
   ],
   "id": "49597647cffe328a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:37:08.086356Z",
     "start_time": "2024-05-03T14:37:04.423145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train =pd.read_csv(\"mnist_train.csv\")\n",
    "y_train = x_train['label']\n",
    "del x_train['label']\n",
    "# Loading mnist test dataset and dividing it into x_train and y train\n",
    "x_test  =pd.read_csv(\"mnist_test.csv\")\n",
    "y_test  = x_test['label']\n",
    "del x_test['label']"
   ],
   "id": "e658c29e8acacd0c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:37:08.101960Z",
     "start_time": "2024-05-03T14:37:08.088355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ],
   "id": "e7fcee8530cbe37f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#......................using all features.......................\n",
    "# Linear Support Vector Machine (SVM) with all the 784 pixels of the MNIST images is used.\n",
    "# a pipeline is set up  where scale is first applied, and then the classifier"
   ],
   "id": "48d61de43c6f4a59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:45:49.763923Z",
     "start_time": "2024-05-03T14:37:08.103974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steps = [('scaling', StandardScaler()), ('clf', SVC())]\n",
    "pipeline = Pipeline(steps)\n",
    "# train\n",
    "t0 = time()\n",
    "pipeline.fit(x_train, y_train)\n",
    "# predict\n",
    "y_pred = pipeline.predict(x_test)\n",
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "# confusion matrix\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "# time taken\n",
    "t_all_feats = time() - t0\n",
    "print(\"Training and classification done in {}s\".format(t_all_feats))"
   ],
   "id": "1561aca44d45029a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.966 \n",
      "\n",
      "[[ 968    0    1    1    0    3    3    2    2    0]\n",
      " [   0 1127    3    0    0    1    2    0    2    0]\n",
      " [   5    1  996    2    2    0    1   15    9    1]\n",
      " [   0    0    4  979    1    7    0   12    7    0]\n",
      " [   0    0   12    0  944    2    4    7    3   10]\n",
      " [   2    0    1   10    2  854    6    8    7    2]\n",
      " [   6    2    1    0    4    8  930    2    5    0]\n",
      " [   1    6   13    2    3    0    0  990    0   13]\n",
      " [   3    0    4    6    6    9    3   14  926    3]\n",
      " [   4    6    5   11   12    2    0   20    3  946]]\n",
      "Training and classification done in 521.6522998809814s\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#.........................using PCA..................................................\n",
    "# The next step is to train and predict using a dataset reduced with PCA,\n",
    "# the number of components for the PCA model is reduced to 50.\n",
    "# define pipeline steps"
   ],
   "id": "f53e0919af74033d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:47:00.013457Z",
     "start_time": "2024-05-03T14:45:49.765925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steps = [('scaling', StandardScaler()), ('reduce_dim', PCA(n_components=50)), ('clf', SVC())]\n",
    "pipeline = Pipeline(steps)\n",
    "# train\n",
    "t0 = time()\n",
    "pipeline.fit(x_train, y_train)\n",
    "# predict\n",
    "y_pred = pipeline.predict(x_test)\n",
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n",
    "# confusion matrix\n",
    "print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "t_reduced_feats = time() - t0\n",
    "print(\"Training and classification done in {}s\".format(t_reduced_feats))\n",
    "print(\"Speedup {}x\".format(t_all_feats/t_reduced_feats))"
   ],
   "id": "6ce163924d110031",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9715 \n",
      "\n",
      "[[ 970    0    1    1    0    3    3    1    1    0]\n",
      " [   0 1127    4    1    0    1    1    0    1    0]\n",
      " [   4    0 1007    3    1    1    1    9    5    1]\n",
      " [   0    1    0  985    1    6    0    9    6    2]\n",
      " [   0    0    7    1  951    0    4    4    2   13]\n",
      " [   2    0    0   14    1  863    6    0    5    1]\n",
      " [   4    3    1    1    4    7  934    1    3    0]\n",
      " [   2    8   14    1    3    0    0  983    3   14]\n",
      " [   3    0    2    9    5    5    2    5  939    4]\n",
      " [   3    5    1    8   13    2    0   14    7  956]]\n",
      "Training and classification done in 70.22853541374207s\n",
      "Speedup 7.427925084977728x\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#...................................Discussion..............................\n",
    "# We get >7x speedup when preprocessing with PCA and an accuracy score\n",
    "# that's quite comparable to having the whole dataset."
   ],
   "id": "1e043739ae2c697"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
